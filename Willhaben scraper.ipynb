{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Following code is for scraping the Austrian online marketplace called Willhaben.at\n",
    "The code scrapes through the website using selenium and soup.\n",
    "For some reason Selenium is better at scraping the Price and Place info than Soup\n",
    "The script can scrape listings as long as there is only one page for a keyword\n",
    "\n",
    "The only missing feature is the ability to go through multiple pages\n",
    "\n",
    "The code is written by matyasmarton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Libraries and define the base url and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base_url contains the link, but the keywords are replaced with {0}\n",
    "base_url = \"https://www.willhaben.at/iad/kaufen-und-verkaufen/marktplatz?keyword={0}&attribute_tree_level_0=&attribute_tree_level_1=&typedKeyword={0}\"\n",
    "# This is the list of keywords the script will go through\n",
    "keywords = ['sony+fs','ursa+mini','red+camera','arri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visited https://www.willhaben.at/iad/kaufen-und-verkaufen/marktplatz?keyword=sony+fs&attribute_tree_level_0=&attribute_tree_level_1=&typedKeyword=sony+fs\n",
      "visited https://www.willhaben.at/iad/kaufen-und-verkaufen/marktplatz?keyword=ursa+mini&attribute_tree_level_0=&attribute_tree_level_1=&typedKeyword=ursa+mini\n",
      "visited https://www.willhaben.at/iad/kaufen-und-verkaufen/marktplatz?keyword=red+camera&attribute_tree_level_0=&attribute_tree_level_1=&typedKeyword=red+camera\n",
      "visited https://www.willhaben.at/iad/kaufen-und-verkaufen/marktplatz?keyword=arri&attribute_tree_level_0=&attribute_tree_level_1=&typedKeyword=arri\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "#Empty lists are created, these will be appended by the for loop\n",
    "\n",
    "name_text = []\n",
    "price_text = []\n",
    "place_text = []\n",
    "date_text = []\n",
    "# Create the columns in the dataframe\n",
    "df = pd.DataFrame( columns = ['Names','Prices','Dates','Places'])\n",
    "\n",
    "\n",
    "try:\n",
    "# First we format the url to contain the keywords    \n",
    "    for keyword in keywords:\n",
    "        url = base_url.format(keyword)\n",
    "        driver = webdriver.Chrome(executable_path = r\"C:\\Users\\Matyas\\Documents\\Untitled Folder\\webdriver\\chromedriver.exe\")\n",
    "# with requests, soup and selenium we open the browser and start scraping the site\n",
    "        r = requests.get(url)\n",
    "        driver.get(url)\n",
    "        html = r.content\n",
    "        soup = BeautifulSoup(html, \"html5lib\")\n",
    "# Willhaben has a popup which needs to be accepted with Selenium\n",
    "        button = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.ID, \"didomi-notice-agree-button\")))\n",
    "        button.click()\n",
    "\n",
    "# We start scraping the site first for name, then price, date and place.\n",
    "\n",
    "        names = soup.find_all(\"span\", {\"itemprop\":\"name\"})\n",
    "# After the scraper obtained the data from the HTML markup we strip it from the tags and any whitespace \n",
    "        for name in names:\n",
    "            name_stripped = name.text.strip()\n",
    "# Then the script appends the scraped and clean data to an empty list\n",
    "            name_text.append(name_stripped)\n",
    "            \n",
    "\n",
    "        prices = WebDriverWait(driver, 15).until(\n",
    "        EC.presence_of_all_elements_located((By.CLASS_NAME, \"info-2-price\")))\n",
    "        for price in prices:\n",
    "            price_stripped = price.text.strip()\n",
    "            price_text.append(price_stripped)\n",
    "        \n",
    "       \n",
    "            \n",
    "        dates = soup.find_all(\"div\",{\"class\":\"bottom-content\"})\n",
    "        for date in dates:\n",
    "            date_stripped = date.text.strip()\n",
    "            date_text.append(date_stripped)\n",
    "\n",
    "            \n",
    "        places = WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"address-lg.w-brk-ln-1 \")))\n",
    "        for place in places:\n",
    "            place_strip = place.text.strip()\n",
    "            place_text.append(place_strip)\n",
    "# After one website is done we print \"visited {url of the website}\"\n",
    "        print(f\"visited {url}\")\n",
    "\n",
    "        \n",
    "finally:\n",
    "# After the scraper got everything it pastes the data to the pandas dataframe\n",
    "    df['Names'] = name_text\n",
    "    df['Prices'] = price_text\n",
    "    df['Dates'] = date_text\n",
    "    df['Places'] = place_text\n",
    "# Then selenium closes the webdriver\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
